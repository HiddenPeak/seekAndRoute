# A6000NVLink 14900K 机器配置大语言模型推理框架记录

## 一、安装Nvidia GPU驱动

https://www.nvidia.com/en-us/drivers/

驱动目前有三种类型：

1. 生产环境使用的稳定版本
2. 最新的实验版本
3. 最新修复漏洞的beta版本

如果需要使用最新稳定版本的cuda编译器的话需要使用beta版本的驱动

``` bash
nvidia-smi
```

终端显示如下：

``` bash
Sun Dec  8 18:53:44 2024       
+-----------------------------------------------------------------------------------------+
| NVIDIA-SMI 560.35.05              Driver Version: 560.35.05      CUDA Version: 12.6     |
|-----------------------------------------+------------------------+----------------------+
| GPU  Name                 Persistence-M | Bus-Id          Disp.A | Volatile Uncorr. ECC |
| Fan  Temp   Perf          Pwr:Usage/Cap |           Memory-Usage | GPU-Util  Compute M. |
|                                         |                        |               MIG M. |
|=========================================+========================+======================|
|   0  NVIDIA RTX A6000               Off |   00000000:01:00.0 Off |                  Off |
| 30%   36C    P8             19W /  300W |     258MiB /  49140MiB |      0%      Default |
|                                         |                        |                  N/A |
+-----------------------------------------+------------------------+----------------------+
|   1  NVIDIA RTX A6000               Off |   00000000:02:00.0 Off |                  Off |
| 30%   34C    P8             22W /  300W |      15MiB /  49140MiB |      0%      Default |
|                                         |                        |                  N/A |
+-----------------------------------------+------------------------+----------------------+
                                                                                         
+-----------------------------------------------------------------------------------------+
| Processes:                                                                              |
|  GPU   GI   CI        PID   Type   Process name                              GPU Memory |
|        ID   ID                                                               Usage      |
|=========================================================================================|
|    0   N/A  N/A      2279      G   /usr/lib/xorg/Xorg                             64MiB |
|    0   N/A  N/A      2854      G   /usr/bin/gnome-shell                          154MiB |
|    1   N/A  N/A      2279      G   /usr/lib/xorg/Xorg                              4MiB |
+-----------------------------------------------------------------------------------------+
```

## 二、安装Cuda-toolkit

https://developer.nvidia.com/cuda-toolkit

1. 添加包管理器源或者下载本地安装包
2. 安装cuda选择需要的版本

安装最新版本的cuda之后，需要手动配置环境变量以保证系统能够正常调用

``` bash
# 添加环境变量 注意版本号要一致
export PATH=/usr/local/cuda-12.6/bin${PATH:+:${PATH}}
# 查看版本是否匹配
nvcc -V
```

终端显示如下：

``` bash
nvcc: NVIDIA (R) Cuda compiler driver
Copyright (c) 2005-2024 NVIDIA Corporation
Built on Tue_Oct_29_23:50:19_PDT_2024
Cuda compilation tools, release 12.6, V12.6.85
Build cuda_12.6.r12.6/compiler.35059454_0
```

版本符合要求就可以添加环境变量.bashrc中

``` bash
sudo nano ~/.bashrc
```

## 三、安装NCCL

https://docs.nvidia.com/deeplearning/nccl/install-guide/index.html

``` bash
wget https://developer.download.nvidia.com/compute/cuda/repos/ubuntu2404/x86_64/cuda-keyring_1.1-1_all.deb
sudo dpkg -i cuda-keyring_1.1-1_all.deb
sudo apt-get update
```

添加安装源后：

``` bash
sudo apt install libnccl2=2.23.4-1+cuda12.6 libnccl-dev=2.23.4-1+cuda12.6
```

根据操作系统和版本选择下载或网络安装libnccl2 libnccl-dev即可。

如果考虑安装NVIDIA HPC SDK则可以不需要如上单独安装

``` bash
# NVIDIA HPC SDK Install
curl https://developer.download.nvidia.com/hpc-sdk/ubuntu/DEB-GPG-KEY-NVIDIA-HPC-SDK | sudo gpg --dearmor -o /usr/share/keyrings/nvidia-hpcsdk-archive-keyring.gpg
echo 'deb [signed-by=/usr/share/keyrings/nvidia-hpcsdk-archive-keyring.gpg] https://developer.download.nvidia.com/hpc-sdk/ubuntu/amd64 /' | sudo tee /etc/apt/sources.list.d/nvhpc.list
sudo apt-get update -y
sudo apt-get install -y nvhpc-24-11
```

安装完成后可以使用NCCL-test对卡间通讯进行测试

https://github.com/NVIDIA/nccl-tests



## 四、安装conda

https://docs.anaconda.com/miniconda/install/#quick-command-line-install

可以使用如下命令行安装

``` bash
mkdir -p ~/miniconda3
wget https://repo.anaconda.com/miniconda/Miniconda3-latest-Linux-x86_64.sh -O ~/miniconda3/miniconda.sh
bash ~/miniconda3/miniconda.sh -b -u -p ~/miniconda3
rm ~/miniconda3/miniconda.sh
```

完成后激活环境变量

``` bash
source ~/miniconda3/bin/activate
```

初始化conda，打开所有shell时自动激活

``` bash
conda init --all
```

## 五、安装vLLM

``` bash
# 创建vllm运行的虚拟环境
conda create -n vllm python=3.12 -y
conda activate vllm

# cuda版本12.1以后的直接安装
pip install vllm
```

vLLM官方建议使用pip进行安装，否则会导致vLLM在调用NCCL时候出现问题。

其他安装需求可以参考：

https://docs.vllm.ai/en/latest/getting_started/installation.html

针对生产环境，不建议使用快速开始的方式进行环境配置。

根据不同的设备，选择不同的编译方式或者wheels安装方式

当需要考虑载入较大的LLM模型时，使用docker镜像方式部署需要设置可用内存大小以及共享内存大小。

## 六、加速模型下载

安装modelscope库

``` bash
pip install modelscope
```

使用命令行下载到指定文件夹

```bash
modelscope download --model OpenGVLab/InternVL2_5-78B --local_dir ./dir
```

如果不采用--local_dir参数则会直接下载到modelscope的默认文件夹，由于需要统一管理，所以要求指定文件夹。

## 七、nvitop安装

``` bash
# 安装
pip3 install --upgrade nvitop

# 运行
nvitop -m full
```

## 八、Tmux安装

安装Tmux

``` bash
sudo apt update
sudo apt install tmux
```

创建Tmux窗口

``` bash
tmux new -s vllm-inference-serving
```

快捷键参考

- `Ctrl+b %`：划分左右两个窗格。
- `Ctrl+b "`：划分上下两个窗格。
- `Ctrl+b <arrow key>`：光标切换到其他窗格。`<arrow key>`是指向要切换到的窗格的方向键，比如切换到下方窗格，就按方向键`↓`。
- `Ctrl+b ;`：光标切换到上一个窗格。
- `Ctrl+b o`：光标切换到下一个窗格。
- `Ctrl+b {`：当前窗格与上一个窗格交换位置。
- `Ctrl+b }`：当前窗格与下一个窗格交换位置。
- `Ctrl+b Ctrl+o`：所有窗格向前移动一个位置，第一个窗格变成最后一个窗格。
- `Ctrl+b Alt+o`：所有窗格向后移动一个位置，最后一个窗格变成第一个窗格。
- `Ctrl+b x`：关闭当前窗格。
- `Ctrl+b !`：将当前窗格拆分为一个独立窗口。
- `Ctrl+b z`：当前窗格全屏显示，再使用一次会变回原来大小。
- `Ctrl+b Ctrl+<arrow key>`：按箭头方向调整窗格大小。
- `Ctrl+b q`：显示窗格编号。

参考网页 https://www.ruanyifeng.com/blog/2019/10/tmux.html

## 九、LLM加载

考虑使用Qwen2.5系列LLM进行测试

### （一）投机解码

草稿模型采用Qwen2.5-7B-Instruct-AWQ，主要模型Qwen2.5-72B-Instruct-AWQ

``` bash
# 从任意虚拟环境注销
conda deactivate
# 激活vllmV1的虚拟环境
conda activate vllm
# 拉起服务
vllm serve Qwen2.5-72B-Instruct-AWQ \
--served-model-name "Qwen2.5-A6000NVLink" \
--host 0.0.0.0 \
--port 58001 \
--api-key testAK \
--task generate \
--download-dir ~/models/llm/ \
--max-model-len 10240 \
--tensor-parallel-size 2 \
--pipeline-parallel-size 1 \
--enable-prefix-caching \
--gpu-memory-utilization 0.7 \
--speculative-model Qwen2.5-7B-Instruct-AWQ \
--num-speculative-tokens 5 \
--speculative-draft-tensor-parallel-size 2 \
--compilation-config 3 
```

**--enable-chunked-prefill** 无法与投机解码**--speculative-draft-tensor-parallel-size**兼容

使用非量化模型推理速度会得到更大提升，vLLM对于AWQ量化的支持并不好，经过2024年11月的测试，GPTQ和AWQ都性能都不太好，但是Int4量化的精度下降尚在可以接受的程度。

根据实际测试调整草稿模型

### （二）普通推理

``` bash
# 从任意虚拟环境注销
conda deactivate
# 激活vllmV1的虚拟环境
conda activate vllmV1
# 拉起服务
vllm serve Qwen2.5-72B-Instruct-AWQ \
--served-model-name "Qwen2.5-A6000NVLink" \
--host 0.0.0.0 \
--port 58001 \
--api-key testAK \
--task generate \
--download-dir ~/models/llm/ \
--max-model-len 10240 \
--tensor-parallel-size 2 \
--pipeline-parallel-size 1 \
--enable-prefix-caching \
--enable-chunked-prefill \
--gpu-memory-utilization 0.7 \
--compilation-config 3 
```

使用以上配置在vLLM V1 dev版本下平均32 tokens/s，0.6.4p1版本速度为29 ts/s，0.5.4版本速度为24 ts/s

## 十、embedding 模型加载

``` bash
# 从任意虚拟环境注销
conda deactivate
# 激活vllmV1的虚拟环境
conda activate vllm
# 拉起服务
vllm serve bce-embedding-base_v1 \
--served-model-name "bce-embedding-base_v1-A6000NVLink" \
--host 0.0.0.0 \
--port 58002 \
--api-key testAK \
--task embedding \
--download-dir ~/models/embedding/ \
--max-model-len 512 \
--tensor-parallel-size 2 \
--pipeline-parallel-size 1 \
--enable-prefix-caching \
--gpu-memory-utilization 0.1
```

gte-Qwen2-1.5B-instruct模型占用大约5GB显存，配置用量10%，实际用量4.8%

如果LLM上下文不够，可以考虑该模型量化，或将部分权重装载到CPU内存，并要考虑部分预留显存。

## 十一、reranker 模型加载

``` bash
# 从任意虚拟环境注销
conda deactivate
# 激活vllmV1的虚拟环境
conda activate vllm
# 拉起服务
vllm serve bge-reranker-v2-m3 \
--served-model-name "bge-reranker-v2-m3-A6000NVLink" \
--host 0.0.0.0 \
--port 58003 \
--api-key testAK \
--download-dir ~/models/llm/ \
--tensor-parallel-size 2 \
--pipeline-parallel-size 1 \
--enable-prefix-caching \
--gpu-memory-utilization 0.1 \
--trust-remote-code \
--compilation-config 3 
```

当前加载的模型需要通过vLLM指定的url路由访问，与dify兼容性差。等待后续完善
